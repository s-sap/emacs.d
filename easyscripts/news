#!/usr/bin/env python3
import requests
import sys
import re, base64
import subprocess
from bs4 import BeautifulSoup


def extractLinks(TEXT):
    # Decode only valid Links
    try:
        # Extract only the base64 string
        _temp1 = re.split('/', TEXT)
        _temp2 = _temp1[-1]+"?"
        _temp2 = _temp2.split('?')[0]
        # Decode & Add padding
        decodeStr = str(base64.b64decode(_temp2 + "=="))
        # Regex find full url
        XX = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', decodeStr)
        # Get the news link
        return XX[0].split('\\')[0]
    except Exception:
        pass

def createHTMLPage(NEWS, PAGE="/tmp/news.html"):
    F = open(PAGE, "w")
    F.writelines(f"<h2 style='color:#aaa'> <center> Query: {' '.join(sys.argv[1:])}  |  {len(NEWS)} Stories Found </center></h2><br><br>")
    F.writelines("<style> body{background-color:#1a1c23} </style><body>")
    indexN=0
    for n in NEWS:
        F.write(f"""<div style='background-color:#1e2028;border-radius:8px;padding:5px;'><center><a href='{n[-1]}' target='_blank' style='color:#aaa;'><h2><span style="float:left;color:#606060;font-size:13px;">[ {indexN} ]</span><span style="float:left;margin-left:2%;color:red;font-size:19px;">{n[2]}</span>{n[0]}<span style="float:right;margin-right:8%;color:#aaa;font-size:19px;">{n[1]}</span></h2></a></center></div><br>""")
        indexN+=1
    F.writelines("</body>")
    subprocess.run(['xdg-open', PAGE])

def termRender(Data, Line=10, titleLen=75):
    n = 0
    indexUrl = []
    for i in Data:
        # Align String
        titleFormat = i[0][:titleLen]
        if len(titleFormat) < titleLen:
            spaceToAdd = " "*(titleLen - len(titleFormat))
            titleFormat = titleFormat+spaceToAdd
        # Render Info
        print(f"[ {n} ] {titleFormat : ^15}\t{i[2]}\t{i[1]}")

        indexUrl.append(i[-1])
        n += 1

        if n % Line == 0:
            try:
                choose = list(input(" [ q to Quit ]   [ Enter for More ] \n"))
                choose = [i for i in choose if i != " "]

                if len(choose) == 0: pass
                elif choose[0] == "q": exit()                
                
                if len(choose) > 1:
                    for c in choose: subprocess.run(['xdg-open', indexUrl[int(c)]])
                elif len(choose) == 1:
                    subprocess.run(['xdg-open', indexUrl[int(choose[0])]])
                    
                indexUrl.clear()
                n = 0
                
            except Exception as e:
                print('Error', e)
                exit()

def newToFile(dataArr, numOfStories=50, FILE="/home/ss/.emacs.d/easyscripts/news.data"):
    ''' Get News Data and Write to a file with seperator '''
    try:
        with open(FILE, "w") as F:            
            for d in dataArr[:numOfStories]:
                if d[-1] is None: d[-1] = ""    # Replace None
                Stories = "||".join(d)          # Join with '||' seperator                
                F.write(f"{Stories}\n")        #  Write to a file
                
    except Exception as e:
        print(e)
        pass

                    
class GNews:
    def __init__(self, Topic):        
        self.Url = f"https://news.google.com/search?q={Topic}&hl=en-US&gl=US&ceid=US%3Aen"
        
    def News(self):
        try:
            s = requests.session()
            headers = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:111.0)'\
                       'Gecko/20100101 Firefox/111.0',
                       'Accept': 'text/html,application/xhtml+xml,application/xml;'\
                       'q=0.9,image/webp,*/*;q=0.8'}
            cookies = {'CONSENT': 'YES+'}

            r = s.get(self.Url, headers=headers, cookies=cookies)
            # Return HTML Data
            return r.text
        except Exception as e:
            print(str(e))

    def getNews(self, toSort=True):
        '''
        Use Html parser to get all the needed Data
        Returns a list news storys
        '''
        soup = BeautifulSoup(self.News(), features="html.parser")        
        link = [extractLinks(i['href']) for i in soup.find_all('a', class_="VDXfz")]
        title = [i.text for i in soup.find_all('h3', class_="ipQwMb")]
        source = [i.text for i in soup.find_all('div', class_="wsLqz")]
        time = [i.text for i in soup.find_all('time', class_="WW6dff")]        

        dicData = {"AllNews":[]}
        for i in range(len(link)): dicData["AllNews"].append([title[i], source[i], time[i], link[i]])
        if toSort:
            # Minutes, Hours, Days old News stories
            all_d, list_d, list_hr, list_min = [],[],[],[]
            for i in dicData["AllNews"]:
                if "minute" in i[2] or "minutes" in i[2]: list_min.append(i)
                elif "hour" in i[2] or "hours" in i[2]: list_hr.append(i)
                elif "Yesterday" in i[2] or "yesterday" in i[2]: list_d.append(i)
                else: all_d.append(i)
            # Sorted Recent
            return list_min + list_hr + list_d + all_d
        # Unsorted
        return dicData["AllNews"]
    
        
if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("topic", help="Topic to Search", nargs='+')
    parser.add_argument("-t", help="Output in Terminal", action="store_true")
    parser.add_argument("-html", help="Output as Html file", action="store_true")    
    
    args = parser.parse_args()
    topic = "-".join(args.topic)    
    N = GNews(topic).getNews()

    print(len(N))
    
    if args.t: termRender(N)
    elif args.html: createHTMLPage(N)
    else: newToFile(N)
